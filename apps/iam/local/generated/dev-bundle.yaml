apiVersion: v1
kind: Service
metadata:
  labels:
    name: minio
  name: minio
  namespace: default
spec:
  ports:
    - name: minio-minio
      port: 9000
      targetPort: 9000
  selector:
    name: minio
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: minio
  namespace: default
spec:
  minReadySeconds: 10
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: minio
  template:
    metadata:
      labels:
        name: minio
    spec:
      containers:
        - args:
            - mkdir -p /data/cortex && mkdir -p /data/loki && /usr/bin/minio server /data
          command:
            - /bin/bash
            - -c
          env:
            - name: MINIO_ACCESS_KEY
              value: minio
            - name: MINIO_SECRET_KEY
              value: minio123
            - name: MINIO_PROMETHEUS_AUTH_TYPE
              value: public
          image: minio/minio:RELEASE.2025-05-24T17-08-30Z
          imagePullPolicy: IfNotPresent
          name: minio
          ports:
            - containerPort: 9000
              name: minio
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: cortex-config
  namespace: default
data:
  config.yaml: |
    # Configuration for running Cortex in single-process mode.
    # This configuration should not be used in production.
    # It is only for getting started and development.

    # Disable the requirement that every request to Cortex has a
    # X-Scope-OrgID header. `fake` will be substituted in instead.
    auth_enabled: false

    server:
      http_listen_port: 9009
      grpc_listen_port: 9095

      # Configure the server to allow messages up to 100MB.
      grpc_server_max_recv_msg_size: 104857600
      grpc_server_max_send_msg_size: 104857600
      grpc_server_max_concurrent_streams: 1000

    distributor:
      shard_by_all_labels: true
      pool:
        health_check_ingesters: true

    ingester_client:
      grpc_client_config:
        # Configure the client to allow messages up to 100MB.
        max_recv_msg_size: 104857600
        max_send_msg_size: 104857600
        grpc_compression: gzip

    ingester:
      # We want our ingesters to flush chunks at the same time to optimise
      # deduplication opportunities.
      spread_flushes: true
      chunk_age_jitter: 0

      walconfig:
        wal_enabled: true
        recover_from_wal: true
        wal_dir: /tmp/cortex/wal

      lifecycler:
        # The address to advertise for this ingester.  Will be autodiscovered by
        # looking up address on eth0 or en0; can be specified if this fails.
        # address: 127.0.0.1

        # We want to start immediately and flush on shutdown.
        join_after: 0
        min_ready_duration: 0s
        final_sleep: 0s
        num_tokens: 512
        tokens_file_path: /tmp/cortex/wal/tokens

        # Use an in memory ring store, so we don't need to launch a Consul.
        ring:
          kvstore:
            store: inmemory
          replication_factor: 1

    storage:
      engine: blocks

    blocks_storage:
      tsdb:
        dir: /tmp/cortex/tsdb
      bucket_store:
        sync_dir: /tmp/cortex/tsdb-sync

      backend: filesystem
      filesystem:
        dir: /tmp/cortex/blocks

    purger:
      object_store_type: filesystem

    frontend_worker:
      # Configure the frontend worker in the querier to match worker count
      # to max_concurrent on the queriers.
      match_max_concurrent: true

    # Configure the ruler to scan the /tmp/cortex/rules directory for prometheus
    # rules: https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/#recording-rules
    ruler:
      enable_api: true
      enable_sharding: false
      storage:
        type: s3
        s3:
          s3: http://minio:minio123@minio.default.svc.cluster.local:9000
          bucketnames: cortex
          s3forcepathstyle: true

    querier:
      at_modifier_enabled: true
---
apiVersion: v1
kind: Service
metadata:
  labels:
    name: cortex
  name: cortex
  namespace: default
spec:
  ports:
    - name: cortex-http
      port: 9009
      targetPort: 9009
    - name: cortex-grpc
      port: 9095
      targetPort: 9095
  selector:
    name: cortex
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cortex
  namespace: default
spec:
  minReadySeconds: 10
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: cortex
  template:
    metadata:
      annotations:
        cortex-config-hash: 128d9bf6025033e3552baff9d5bb7715
      labels:
        name: cortex
    spec:
      containers:
        - args:
            - -config.file=/etc/cortex/config.yaml
          env:
            - name: JAEGER_AGENT_HOST
              value: tempo
            - name: JAEGER_ENDPOINT
              value: http://agent.default.svc.cluster.local:14268/api/traces
            - name: JAEGER_SAMPLER_TYPE
              value: const
            - name: JAEGER_SAMPLER_PARAM
              value: "1"
          image: cortexproject/cortex:v1.9.0
          imagePullPolicy: IfNotPresent
          name: cortex
          ports:
            - containerPort: 9009
              name: http
            - containerPort: 9095
              name: grpc
          volumeMounts:
            - mountPath: /etc/cortex
              name: cortex-config
      volumes:
        - configMap:
            name: cortex-config
          name: cortex-config
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: cortex-ingress
  namespace: default
spec:
  rules:
    - host: cortex.k3d.localhost
      http:
        paths:
          - backend:
              service:
                name: cortex
                port:
                  number: 9009
            path: /
            pathType: Prefix

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: tempo-config
  namespace: default
data:
  overrides.yaml: |
    overrides:
  tempo.yaml: |
    auth_enabled: false
    server:
      http_listen_port: 3100
    compactor:
      compaction:
        compacted_block_retention: 24h
    distributor:
      receivers:
        jaeger:
          protocols:
            thrift_compact:
              endpoint: 0.0.0.0:6831
        otlp:
          protocols:
            grpc:
              endpoint: 0.0.0.0:4317
    ingester: {}
    storage:
      trace:
        backend: local
        search:
        local:
          path: /tmp/tempo/traces
        wal:
          path: /tmp/tempo/wal
          v2_encoding: none
          search_encoding: none
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tempo
  namespace: default
spec:
  minReadySeconds: 10
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: tempo
  template:
    metadata:
      annotations:
        tempo-config-hash: 1a64b8ef2f49f5c9e8057582936f2f7b
      labels:
        name: tempo
    spec:
      containers:
      - args:
        - -config.file=/conf/tempo.yaml
        - -mem-ballast-size-mbs=1024
        image: grafana/tempo:2.2.0
        imagePullPolicy: IfNotPresent
        name: tempo
        ports:
        - containerPort: 3100
          name: http
        - containerPort: 4317
          name: otlp-http
        volumeMounts:
        - mountPath: /conf
          name: tempo-config
      volumes:
      - configMap:
          name: tempo-config
        name: tempo-config
---
apiVersion: v1
kind: Service
metadata:
  labels:
    name: tempo
  name: tempo
  namespace: default
spec:
  ports:
  - name: tempo-http
    port: 3100
    targetPort: 3100
  - name: tempo-otlp-http
    port: 4317
    targetPort: 4317
  selector:
    name: tempo
---
apiVersion: v1
data:
  agent.yml: |
    logs:
      configs:
        - name: k8sevents
          clients:
          - url: http://loki.default.svc.cluster.local:3100/loki/api/v1/push
            external_labels:
              cluster: my-cluster
              job: integrations/kubernetes/eventhandler
          positions:
            filename: /tmp/k8seventspositions.yaml
        - name: agent
          clients:
            - url: http://loki.default.svc.cluster.local:3100/loki/api/v1/push
              external_labels:
                cluster: my-cluster
          positions:
            filename: /tmp/positions.yaml
          target_config:
            sync_period: 10s
          scrape_configs:
          - job_name: integrations/kubernetes/pod-logs
            kubernetes_sd_configs:
              - role: pod
            pipeline_stages:
              - cri: {}
            relabel_configs:
              - source_labels:
                  - __meta_kubernetes_pod_label_name
                  - __meta_kubernetes_pod_label_app_kubernetes_io_name # also
                target_label: __service__
                separator: ""
              - source_labels:
                  - __meta_kubernetes_pod_node_name
                target_label: __host__
              - action: drop
                regex: ""
                source_labels:
                  - __service__
              - action: labelmap
                regex: __meta_kubernetes_pod_label_(.+)
              - action: replace
                replacement: $1
                separator: /
                source_labels:
                  - __meta_kubernetes_namespace
                  - __service__
                target_label: job
              - action: replace
                source_labels:
                  - __meta_kubernetes_namespace
                target_label: namespace
              - action: replace
                source_labels:
                  - __meta_kubernetes_pod_name
                target_label: pod
              - action: replace
                source_labels:
                  - __meta_kubernetes_pod_container_name
                target_label: container
              - replacement: /var/log/pods/*$1/*.log
                separator: /
                source_labels:
                  - __meta_kubernetes_pod_uid
                  - __meta_kubernetes_pod_container_name
                target_label: __path__
              - action: replace # `cluster` label is required for the mimir integration, so set a default value
                regex: ""
                replacement: k8s-cluster
                separator: ""
                source_labels:
                  - cluster
                target_label: cluster
              - action: replace
                source_labels:
                  - __meta_kubernetes_pod_container_name
                target_label: name
    metrics:
      global:
        scrape_interval: 15s
        remote_write:
        - url: "http://cortex.default.svc.cluster.local:9009/api/v1/push"
      configs:
        - name: default
          scrape_configs:
          - job_name: grafana
            static_configs:
              - targets: ['grafana.default.svc.cluster.local']
          - job_name: operator
            static_configs:
              - targets: ['iam-app-operator.default.svc.cluster.local:9090']
      wal_directory: /var/lib/agent/data
    server:
        log_level: info
    traces:
        configs:
            - batch:
                send_batch_size: 1000
                timeout: 5s
              name: traces_default
              receivers:
                otlp:
                  protocols:
                    grpc:
                jaeger:
                    protocols:
                        thrift_compact: null
                        thrift_http: null
              remote_write:
                - endpoint: tempo.default.svc.cluster.local:4317
                  insecure: true
                  retry_on_failure:
                    enabled: true
                    initial_interval: 500ms
                    max_elapsed_time: 1s
kind: ConfigMap
metadata:
  name: grafana-agent-config
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: grafana-agent
rules:
- apiGroups:
  - ""
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs:
  - get
  - list
  - watch
- nonResourceURLs:
  - /metrics
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: grafana-agent
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: grafana-agent
subjects:
- kind: ServiceAccount
  name: grafana-agent
  namespace: default
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: grafana-agent
  namespace: default
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana-agent
  namespace: default
spec:
  minReadySeconds: 10
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: grafana-agent
  template:
    metadata:
      annotations:
        grafana-agent-config-hash: ea051677fc887c7c7d9894f7d450fdb1
      labels:
        name: grafana-agent
    spec:
      containers:
      - args:
        - -config.file=/etc/agent/agent.yml
        - -server.http.address=0.0.0.0:80
        env:
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        image: grafana/agent:v0.35.2
        imagePullPolicy: IfNotPresent
        name: agent
        ports:
        - containerPort: 6831
          name: thrift-compact
          protocol: UDP
        - containerPort: 14268
          name: thrift-http
          protocol: TCP
        - containerPort: 4317
          name: otlp-grpc
          protocol: TCP
        volumeMounts:
        - mountPath: /etc/agent
          name: grafana-agent-config
        - mountPath: /var/log
          name: varlogs
          readOnly: true
      serviceAccount: grafana-agent
      volumes:
      - configMap:
          name: grafana-agent-config
        name: grafana-agent-config
      - hostPath:
          path: /var/log
        name: varlogs
---
apiVersion: v1
kind: Service
metadata:
  labels:
    name: grafana-agent
  name: grafana-agent
  namespace: default
spec:
  ports:
  - name: agent-thrift-compact
    port: 6831
    protocol: UDP
    targetPort: 6831
  - name: agent-thrift-http
    port: 14268
    protocol: TCP
    targetPort: 14268
  - name: agent-otel-grpc
    port: 4317
    protocol: TCP
    targetPort: 4317
  selector:
    name: grafana-agent

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: loki-config
  namespace: default
data:
  config.yaml: |
    auth_enabled: false
    server:
      http_listen_port: 3100
    common:
      path_prefix: /tmp/loki
      storage:
        filesystem:
          chunks_directory: /tmp/loki/chunks
          rules_directory: /tmp/loki/rules
      replication_factor: 1
      ring:
        instance_addr: 127.0.0.1
        kvstore:
          store: inmemory
    schema_config:
      configs:
        - from: 2020-10-24
          store: boltdb-shipper
          object_store: filesystem
          schema: v11
          index:
            prefix: index_
            period: 24h
    ruler:
      alertmanager_url: http://localhost:9093
      storage:
        type: s3
        s3:
          s3: http://minio:minio123@minio.default.svc.cluster.local:9000
          bucketnames: loki
          s3forcepathstyle: true
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: loki
  namespace: default
spec:
  minReadySeconds: 10
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: loki
  template:
    metadata:
      annotations:
        loki-config-hash: 3ea717a076077bf16353d56e9f79510d
      labels:
        name: loki
    spec:
      containers:
      - args:
        - -config.file=/etc/loki/config.yaml
        env:
        - name: JAEGER_AGENT_HOST
          value: tempo
        - name: JAEGER_ENDPOINT
          value: http://agent.default.svc.cluster.local:14268/api/traces
        - name: JAEGER_SAMPLER_TYPE
          value: const
        - name: JAEGER_SAMPLER_PARAM
          value: "1"
        image: grafana/loki:2.8.4
        imagePullPolicy: IfNotPresent
        name: loki
        ports:
        - containerPort: 3100
          name: http
        - containerPort: 9095
          name: grpc
        volumeMounts:
        - mountPath: /etc/loki
          name: loki-config
      volumes:
      - configMap:
          name: loki-config
        name: loki-config
---
apiVersion: v1
kind: Service
metadata:
  labels:
    name: loki
  name: loki
  namespace: default
spec:
  ports:
  - name: loki-http
    port: 3100
    targetPort: 3100
  - name: loki-grpc
    port: 9095
    targetPort: 9095
  selector:
    name: loki
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: loki-ingress
  namespace: default
spec:
  rules:
  - host: loki.k3d.localhost
    http:
      paths:
      - backend:
          service:
            name: loki
            port:
              number: 3100
        path: /
        pathType: Prefix
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: grafana
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: job-checker
rules:
- apiGroups: ["batch"]
  resources: ["jobs"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: grafana:job-checker-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: job-checker
subjects:
  - kind: ServiceAccount
    name: grafana
    namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: grafana:user-user-rolebinding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: user-user
subjects:
  - kind: ServiceAccount
    name: grafana
    namespace: default
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-config
  namespace: default
  labels:
    name: grafana
data:
  grafana.ini: |
    app_mode = development
    target = all

    [auth.anonymous]
    enabled = false

    [server]
    http_port = 3000
    protocol = https
    router_logging = true
    enforce_domain = false

    [feature_toggles]
    grafanaAPIServerWithExperimentalAPIs = true
    grafanaAPIServerEnsureKubectlAccess = true
    unifiedStorageSearchUI = true
    unifiedStorageSearch = true

    [grafana-apiserver]
    storage_type = unified
    storage_path = /etc/grafana-kubeconfig

    [unified_storage.folders.folder.grafana.app]
    dualWriterMode = 2

    [unified_storage.dashboards.dashboard.grafana.app]
    dualWriterMode = 2
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: grafana-kubeconfig-pvc
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Mi
---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: default
  labels:
    name: grafana
spec:
  ports:
    - name: grafana-https
      port: 443
      targetPort: 3000
      nodePort: 30443
      protocol: TCP
    - name: grafana-apiserver
      port: 6443
      targetPort: 6443
      protocol: TCP
  selector:
    name: grafana
  type: NodePort
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: default
spec:
  minReadySeconds: 10
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: grafana
  template:
    metadata:
      labels:
        name: grafana
    spec:
      containers:
        - env:
            - name: GF_INSTALL_PLUGINS
              value: ""
            - name: GF_PATHS_CONFIG
              value: /etc/grafana-config/grafana.ini
          image: grafana/grafana-enterprise:latest
          imagePullPolicy: IfNotPresent
          name: grafana
          ports:
            - containerPort: 3000
              name: grafana-metrics
          resources:
            requests:
              cpu: 10m
              memory: 40Mi
          volumeMounts:
            - mountPath: /etc/grafana-config
              name: grafana-config
            - mountPath: /etc/grafana/provisioning/datasources
              name: grafana-datasources
            - mountPath: /etc/grafana-kubeconfig
              name: grafana-kubeconfig
      serviceAccount: grafana
      volumes:
        - configMap:
            name: grafana-config
          name: grafana-config
        - configMap:
            name: grafana-datasources
          name: grafana-datasources
        - name: grafana-kubeconfig
          persistentVolumeClaim:
            claimName: grafana-kubeconfig-pvc
---
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    name: grafana
  name: grafana-datasources
  namespace: default
data:
  Cortex.yml: |
    apiVersion: 1
    datasources:
        - access: proxy
          editable: false
          isDefault: true
          name: grafana-k3d-cortex-prom
          type: prometheus
          uid: grafana-prom-cortex
          url: http://cortex.default.svc.cluster.local:9009/api/prom
          version: 1
        - access: proxy
          editable: false
          isDefault: false
          name: grafana-k3d-tempo
          type: tempo
          uid: grafana-traces-tempo
          url: http://tempo.default.svc.cluster.local:3100
          version: 1
        - access: proxy
          editable: false
          isDefault: false
          name: grafana-k3d-loki
          type: loki
          uid: grafana-logs-loki
          url: http://loki.default.svc.cluster.local:3100
          version: 1

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: operator
  namespace: default
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: iam-app-operator
  namespace: default
spec:
  minReadySeconds: 10
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: iam-app-operator
  template:
    metadata:
      labels:
        name: iam-app-operator
    spec:
      serviceAccount: operator
      containers:
        # Add nginx proxy sidecar
        - name: nginx-proxy
          image: nginx:alpine
          command:
            - /bin/sh
            - -c
            - |
              echo "Creating nginx config for proxy..."
              cat > /etc/nginx/nginx.conf << 'EOF'
              events {
                worker_connections 1024;
              }

              stream {
                upstream grafana_backend {
                  server grafana.default.svc.cluster.local:6443;
                }

                server {
                  listen 6443;
                  listen [::]:6443;
                  proxy_pass grafana_backend;
                  proxy_timeout 10s;
                  proxy_connect_timeout 5s;
                }
              }
              EOF
              echo "Starting nginx proxy..."
              nginx -g 'daemon off;'
          ports:
            - containerPort: 6443
              name: proxy
        - image: localhost/github.com/grafana/grafana/apps/iam/operator:latest
          imagePullPolicy: IfNotPresent
          name: iam-app-operator
          command: ["/bin/sh"]
          args:
            - -c
            - |
              echo "Waiting for kubeconfig file..."
              while [ ! -f /etc/grafana-kubeconfig/grafana.kubeconfig ]; do
                echo "Kubeconfig not found, waiting 5 seconds..."
                sleep 5
              done
              echo "Waiting for proxy to be ready..."
              sleep 10  # Give socat time to start
              echo "Kubeconfig found and proxy ready, starting operator..."
              exec /usr/bin/operator
          env:
            - name: OTEL_HOST
              value: tempo.default.svc
            - name: OTEL_PORT
              value: "4317"
            - name: OTEL_CONN_TYPE
              value: grpc
            - name: OTEL_SERVICE_NAME
              value: "iam-app-operator"
            # TODO: enable for Single Tenant Grafana as the source of folders
            #- name: KUBE_CONFIG_FILE
            #  value: /etc/grafana-kubeconfig/grafana.kubeconfig
            # The following three settings are if you want MT Folder App as the source of folders
            # Useful for testing auth for the deployments
            - name: FOLDER_APP_URL
              value: http://host.docker.internal:6446
            - name: AUTH_TOKEN_EXCHANGE_URL
              value: https://host.docker.internal:6481/sign/access-token
            - name: AUTH_TOKEN
              value: "ThisIsMySecretToken"
          ports:
            - containerPort: 9090
              name: metrics
          volumeMounts:
            - mountPath: /etc/grafana-kubeconfig
              name: grafana-kubeconfig
      volumes:
        - name: grafana-kubeconfig
          persistentVolumeClaim:
            claimName: grafana-kubeconfig-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: iam-app-operator
  namespace: default
spec:
  selector:
    name: iam-app-operator
  ports:
    - name: metrics
      port: 9090
      targetPort: metrics
